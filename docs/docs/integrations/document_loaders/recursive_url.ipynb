{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7cc773",
   "metadata": {},
   "source": [
    "# Recursive URL\n",
    "\n",
    "We may want to process load all URLs under a root directory.\n",
    "\n",
    "For example, let's look at the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n",
    "\n",
    "This has many interesting child pages that we may want to read in bulk.\n",
    "\n",
    "Of course, the `WebBaseLoader` can load a list of pages. \n",
    "\n",
    "But, the challenge is traversing the tree of child pages and actually assembling that list!\n",
    " \n",
    "We do this using the `RecursiveUrlLoader`.\n",
    "\n",
    "This also gives us the flexibility to exclude some children, customize the extractor, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8094f",
   "metadata": {},
   "source": [
    "## Basic Example\n",
    "\n",
    "Let's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n",
    "\n",
    "### Library Installation\n",
    "\n",
    "Before starting let's make sure we have installed the proper libraries to run our code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70eedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67365862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55394afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://docs.python.org/3.9/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35504d4e",
   "metadata": {},
   "source": [
    "Let's examine the metadata of the first document we loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084fb2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://docs.python.org/3.9/',\n",
       " 'content_type': 'text/html',\n",
       " 'title': '3.9.19 Documentation',\n",
       " 'language': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130c2fa",
   "metadata": {},
   "source": [
    "Great! That looks like the root page we started from. Let's look at the meta data of the next document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bd7e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://docs.python.org/3.9/c-api/index.html',\n",
       " 'content_type': 'text/html',\n",
       " 'title': 'Python/C API Reference Manual â€” Python 3.9.19 documentation',\n",
       " 'language': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a4dcf",
   "metadata": {},
   "source": [
    "That url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d46d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n\\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n  <head>\\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    \\n    <link rel=\"stylesheet\" href=\"_static'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87cc239",
   "metadata": {},
   "source": [
    "That certainly looks like HTML that comes from the url https://docs.python.org/3.9/, which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c666c",
   "metadata": {},
   "source": [
    "## More Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41cc89",
   "metadata": {},
   "source": [
    "## Adding an Extractor\n",
    "\n",
    "In the basic example, our documents page content was unfiltered - meaning that it returned raw HTML. In most cases, we would like to extract more useful information. To do this, we can pass an extractor to our `RecursiveUrlLoader` that helps us only use the information we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a6f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7902aac",
   "metadata": {},
   "source": [
    "Now, let's take a look at the contents of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc14c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n3.9.19 Documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload\\nDownload these documents\\nDocs by version\\n\\nPython 3.13 (in development)\\nPython 3.12 (stable)\\nPython 3.11 (security-fixes)\\nPython 3.10 (security-fixes)\\nPython 3.9 (security-fixe'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8a826",
   "metadata": {},
   "source": [
    "As we desired, all the HTML tags have been removed and we are just left with the raw text from the page. You can create your own extractors to only pull the exact information you want from a page. In addition you can pass an extractor to the parameter `metadata_extractor` which is a function that inputs the raw HTML, url, and `requests.Response` and returns a dictionary that is used as the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dddbc94",
   "metadata": {},
   "source": [
    "### Lazy Loading\n",
    "\n",
    "A helpful way to improve performance is to use lazy loading, which alleviates us from having to wait for all our documents to load at once. This will minimize the stress on memory by only loading one URL at a time. Let's take a look at how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d0114fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = RecursiveUrlLoader(\n",
    "    url=url\n",
    ")\n",
    "docs = loader.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae95e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://docs.python.org/3.9/', 'content_type': 'text/html', 'title': '3.9.19 Documentation', 'language': None}\n"
     ]
    }
   ],
   "source": [
    "first_doc = next(docs)\n",
    "print(first_doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c174c",
   "metadata": {},
   "source": [
    "We see that lazy loading is much quicker if we are only interested in the first element, or if we have code that will only require us to look at one website at a time. Instead of storing all of the documents in memory, we will only store a single one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d1c8f",
   "metadata": {},
   "source": [
    "## More Advanced Topics\n",
    "\n",
    "These examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your specific use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted Urls, and combining the `use_async` parameter with method functions `aload()` and `alazy_load()` can allow you to use asynchronous loading for more advanced projects. For more information on these and other parameters, please read the API reference in detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
